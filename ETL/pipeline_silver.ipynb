{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXnnyDS2ldqwVL+z9mNht4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datajoedata/e-Commerce-Sales-Analysis-Target-Brazil/blob/main/pipeline_silver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I - Imports e montagem do drive"
      ],
      "metadata": {
        "id": "B8P2MmRzCia6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile"
      ],
      "metadata": {
        "id": "tqDy-AosJE7y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ9qePkzCjM_",
        "outputId": "0d59fa44-1874-4df6-81a5-91153ec929ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II - Funções Auxiliares\n"
      ],
      "metadata": {
        "id": "iMpjO_0cI0ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descompacta arquivos em uma pasta especificada\n",
        "def descompactar_arquivos(zip_path, extrair_para='/content/'):\n",
        "    with ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extrair_para)\n",
        "    print(f\"Arquivos descompactados em: {extrair_para}\")\n",
        "\n",
        "# Lista arquivos descompactados\n",
        "def listar_arquivos(caminho):\n",
        "    arquivos = os.listdir(caminho)\n",
        "    print(\"Arquivos descompactados:\", arquivos)\n",
        "    return arquivos\n",
        "\n",
        "# Carrega arquivos CSV em DataFrames\n",
        "def carregar_csv(caminho_arquivo, nome_arquivo):\n",
        "    return pd.read_csv(f\"{caminho_arquivo}/{nome_arquivo}\")\n",
        "\n",
        "# Cria uma pasta, se ela não existir\n",
        "def criar_pasta(caminho):\n",
        "    if not os.path.exists(caminho):\n",
        "        os.makedirs(caminho)\n",
        "\n",
        "# Remover uma pasta e seus arquivos\n",
        "def remover_pasta(caminho):\n",
        "    for arquivo in os.listdir(caminho):\n",
        "        os.remove(os.path.join(caminho, arquivo))\n",
        "    os.rmdir(caminho)\n",
        "    print(f\"Pasta '{caminho}' removida.\")"
      ],
      "metadata": {
        "id": "imE-7NQ0G1dS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III - Funções de Limpeza\n"
      ],
      "metadata": {
        "id": "jyeXzyTyJi-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicatas em uma coluna específica\n",
        "def remover_duplicatas(df, coluna):\n",
        "    linhas_antes = len(df)\n",
        "    df_sem_duplicatas = df.drop_duplicates(subset=coluna, keep='first')\n",
        "    linhas_removidas = linhas_antes - len(df_sem_duplicatas)\n",
        "    print(f'Total de linhas removidas com base na coluna \"{coluna}\": {linhas_removidas}')\n",
        "    return df_sem_duplicatas\n",
        "\n",
        "# Limpa colunas string (remover espaços extras)\n",
        "def limpar_strings(df):\n",
        "    return df.apply(lambda x: x.str.strip() if x.dtype == 'object' else x)\n",
        "\n",
        "# Converte colunas para tipos específicos\n",
        "\n",
        "\n",
        "def converter_tipo(df, colunas, tipo, errors='raise'):\n",
        "    for coluna in colunas:\n",
        "        df[coluna] = df[coluna].astype(tipo, errors=errors)\n",
        "    return df\n",
        "\n",
        "# Converte colunas para datetime\n",
        "\n",
        "def converter_para_datetime(df, colunas):\n",
        "    for coluna in colunas:\n",
        "        df[coluna] = pd.to_datetime(df[coluna], errors='coerce')\n",
        "    return df"
      ],
      "metadata": {
        "id": "fxlwupWeG-qe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV - Funções de Salvamento"
      ],
      "metadata": {
        "id": "mzmGU8NqKEtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para salvar DataFrames em formato parquet na pasta especificada\n",
        "def salvar_em_parquet(df, nome_arquivo, caminho_silver):\n",
        "    criar_pasta(caminho_silver)\n",
        "    caminho_completo = f\"{caminho_silver}/{nome_arquivo}.parquet\"\n",
        "    df.to_parquet(caminho_completo, index=False)\n",
        "    print(f\"Arquivo salvo em Parquet: {caminho_completo}\")"
      ],
      "metadata": {
        "id": "-jwgR5HOHIE2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V - Pipeline\n"
      ],
      "metadata": {
        "id": "5jlUtDvdKJvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zHfk5Q_1B1tr"
      },
      "outputs": [],
      "source": [
        "# Processar cada DataFrame (limpeza específica de cada um)\n",
        "def processar_clientes(df):\n",
        "    df = remover_duplicatas(df, 'customer_unique_id')\n",
        "    df = converter_tipo(df, ['customer_zip_code_prefix'], 'str')\n",
        "    return limpar_strings(df)\n",
        "\n",
        "def processar_vendedores(df):\n",
        "    df = remover_duplicatas(df, 'seller_id')\n",
        "    df = converter_tipo(df, ['seller_zip_code_prefix'], 'str')\n",
        "    return limpar_strings(df)\n",
        "\n",
        "def processar_itens_pedidos(df):\n",
        "    df = remover_duplicatas(df, 'order_id')\n",
        "    df = converter_tipo(df, ['order_item_id'], 'str')\n",
        "    return converter_para_datetime(df, ['shipping_limit_date'])\n",
        "\n",
        "def processar_geolocalizacao(df):\n",
        "    df = converter_tipo(df, ['geolocation_zip_code_prefix'], 'str')\n",
        "    return limpar_strings(df)\n",
        "\n",
        "def processar_pagamentos(df):\n",
        "    df = remover_duplicatas(df, 'order_id')\n",
        "    df = limpar_strings(df)\n",
        "    df['payment_type'] = df['payment_type'].str.lower()\n",
        "    return df\n",
        "\n",
        "def processar_pedidos(df):\n",
        "    df = remover_duplicatas(df, 'order_id')\n",
        "    df = converter_para_datetime(df, [\n",
        "        'order_purchase_timestamp', 'order_approved_at',\n",
        "        'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
        "        'order_estimated_delivery_date'\n",
        "    ])\n",
        "    return limpar_strings(df)\n",
        "\n",
        "def processar_produtos(df):\n",
        "    df = remover_duplicatas(df, 'product_id')\n",
        "    df.rename(columns={'product category': 'product_category'}, inplace=True)\n",
        "    df['product_category'] = df['product_category'].str.lower().str.strip()\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI - Função Principal\n"
      ],
      "metadata": {
        "id": "Vv3QxfCUKzTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def executar_pipeline(zip_path, caminho_bronze, caminho_silver):\n",
        "    print(\"\\n=== Início da Pipeline ===\\n\")\n",
        "\n",
        "    # Cria pasta temporária de bronze\n",
        "    criar_pasta(caminho_bronze)\n",
        "\n",
        "    # Descompacta arquivos na pasta bronze e lista arquivos\n",
        "    descompactar_arquivos(zip_path, caminho_bronze)\n",
        "    listar_arquivos(caminho_bronze)\n",
        "\n",
        "    # Carrega DataFrames e aplica processamento específico\n",
        "    print(\"\\n=== Processamento dos DataFrames ===\")\n",
        "    clientes_df = processar_clientes(carregar_csv(caminho_bronze, 'customers.csv'))\n",
        "    vendedores_df = processar_vendedores(carregar_csv(caminho_bronze, 'sellers.csv'))\n",
        "    itens_pedidos_df = processar_itens_pedidos(carregar_csv(caminho_bronze, 'order_items.csv'))\n",
        "    geolocalizacao_df = processar_geolocalizacao(carregar_csv(caminho_bronze, 'geolocation.csv'))\n",
        "    pagamentos_df = processar_pagamentos(carregar_csv(caminho_bronze, 'payments.csv'))\n",
        "    pedidos_df = processar_pedidos(carregar_csv(caminho_bronze, 'orders.csv'))\n",
        "    produtos_df = processar_produtos(carregar_csv(caminho_bronze, 'products.csv'))\n",
        "\n",
        "    # Salva DataFrames processados em formato Parquet na pasta silver com sufixo\n",
        "    print(\"\\n=== Salvando DataFrames em Parquet ===\")\n",
        "    salvar_em_parquet(clientes_df, 'clientes', caminho_silver)\n",
        "    salvar_em_parquet(vendedores_df, 'vendedores', caminho_silver)\n",
        "    salvar_em_parquet(itens_pedidos_df, 'itens_pedidos', caminho_silver)\n",
        "    salvar_em_parquet(geolocalizacao_df, 'geolocalizacao', caminho_silver)\n",
        "    salvar_em_parquet(pagamentos_df, 'pagamentos', caminho_silver)\n",
        "    salvar_em_parquet(pedidos_df, 'pedidos', caminho_silver)\n",
        "    salvar_em_parquet(produtos_df, 'produtos', caminho_silver)\n",
        "\n",
        "    # Remove a pasta temporária de bronze e seus arquivos\n",
        "    remover_pasta(caminho_bronze)\n",
        "\n",
        "    print(\"\\n=== Pipeline Finalizada ===\")"
      ],
      "metadata": {
        "id": "dHD4b-RhKRl3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VII - Execução da Pipeline"
      ],
      "metadata": {
        "id": "VfPB_fpwKRE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/Analise_de_Vendas_E-commerce/archive (2).zip'\n",
        "caminho_bronze = '/content/drive/MyDrive/Analise_de_Vendas_E-commerce/bronze-temp'\n",
        "caminho_silver = '/content/drive/MyDrive/Analise_de_Vendas_E-commerce/silver'\n",
        "\n",
        "# Executa a pipeline completa\n",
        "executar_pipeline(zip_path, caminho_bronze, caminho_silver)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtOqnmAoHPS2",
        "outputId": "1d2a964a-2826-4740-fd6e-0ac65ef8e641"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Início da Pipeline ===\n",
            "\n",
            "Arquivos descompactados em: /content/drive/MyDrive/Analise_de_Vendas_E-commerce/bronze-temp\n",
            "Arquivos descompactados: ['customers.csv', 'geolocation.csv', 'order_items.csv', 'orders.csv', 'payments.csv', 'products.csv', 'sellers.csv']\n",
            "\n",
            "=== Processamento dos DataFrames ===\n",
            "Total de linhas removidas com base na coluna \"customer_unique_id\": 3345\n",
            "Total de linhas removidas com base na coluna \"seller_id\": 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-f4de419b5123>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[coluna] = df[coluna].astype(tipo, errors=errors)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de linhas removidas com base na coluna \"order_id\": 13984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-f4de419b5123>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[coluna] = df[coluna].astype(tipo, errors=errors)\n",
            "<ipython-input-4-f4de419b5123>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[coluna] = pd.to_datetime(df[coluna], errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de linhas removidas com base na coluna \"order_id\": 4446\n",
            "Total de linhas removidas com base na coluna \"order_id\": 0\n",
            "Total de linhas removidas com base na coluna \"product_id\": 0\n",
            "\n",
            "=== Salvando DataFrames em Parquet ===\n",
            "Arquivo salvo em Parquet: /content/drive/MyDrive/Analise_de_Vendas_E-commerce/silver/clientes.parquet\n",
            "Arquivo salvo em Parquet: /content/drive/MyDrive/Analise_de_Vendas_E-commerce/silver/vendedores.parquet\n",
            "Arquivo salvo em Parquet: /content/drive/MyDrive/Analise_de_Vendas_E-commerce/silver/itens_pedidos.parquet\n",
            "Arquivo salvo em Parquet: /content/drive/MyDrive/Analise_de_Vendas_E-commerce/silver/geolocalizacao.parquet\n",
            "Arquivo salvo em Parquet: /content/drive/MyDrive/Analise_de_Vendas_E-commerce/silver/pagamentos.parquet\n",
            "Arquivo salvo em Parquet: /content/drive/MyDrive/Analise_de_Vendas_E-commerce/silver/pedidos.parquet\n",
            "Arquivo salvo em Parquet: /content/drive/MyDrive/Analise_de_Vendas_E-commerce/silver/produtos.parquet\n",
            "Pasta '/content/drive/MyDrive/Analise_de_Vendas_E-commerce/bronze-temp' removida.\n",
            "\n",
            "=== Pipeline Finalizada ===\n"
          ]
        }
      ]
    }
  ]
}
